{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMl+mcZtKhVNKfEdGmEkNQG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaezDiana/cuaderno1/blob/main/land_use_land_cover_espa%C3%B1ol_part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clasificación de Uso y Cobertura del Suelo usando Pytorch\n",
        "**Creadores de Contenido**: Isabelle Tingzon y Ankur Mahesh\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Este notebook es una obra derivada y traducida del notebook: [Land Use Land Cover (LULC)](https://colab.research.google.com/github/climatechange-ai-tutorials/lulc-classification/blob/main/land_use_land_cover_part1.ipynb)"
        "\n",
        "**Traducción**: Sergio Acuña, Diana Paez y Alejandro Piña\n",
        "\n",
        "¡Bienvenido al tutorial de CCAI sobre clasificación de uso y cobertura del suelo (LULC) utilizando Pytorch!\n",
        "\n",
        "En este tutorial de dos partes, aprenderás a:\n",
        "- entrenar un modelo de aprendizaje profundo para clasificación de imágenes usando Pytorch\n",
        "- generar mapas de uso y cobertura del suelo utilizando Python GIS\n",
        "\n",
        "Puedes hacer una copia de este tutorial seleccionando Archivo→Guardar una copia en Drive.\n"
      ],
      "metadata": {
        "id": "FlAPePPcqJkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tabla de Contenidos\n",
        "\n",
        "*   [Visión General](#vision-general)\n",
        "*   [Impacto Climático](#impacto-climatico)\n",
        "*   [Público Objetivo](#publico-objetivo)\n",
        "*   [Antecedentes y Requisitos Previos](#antecedentes-y-requisitos-previos)\n",
        "*   [Requisitos de Software](#requisitos-de-software)\n",
        "*   [Descripción de los Datos](#descripcion-de-los-datos)\n",
        "*   [Metodología](#metodologia)\n",
        "*   [Resultados](#resultados)\n",
        "*   [Ejercicios](#ejercicios)\n",
        "*   [Referencias](#referencias)\n"
      ],
      "metadata": {
        "id": "fF28aMEJqaie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"vision-general\"></a>\n",
        "# Visión General\n",
        "Este tutorial abarca una introducción a la clasificación de imágenes utilizando Pytorch para la creación de mapas de uso y cobertura del suelo (LULC).\n",
        "\n",
        "Específicamente, aprenderás a:\n",
        "- clasificar imágenes satelitales en 10 categorías de LULC utilizando el [conjunto de datos EuroSAT](https://arxiv.org/abs/1709.00029)\n",
        "- ajustar un modelo CNN Resnet-50 para clasificación de imágenes\n",
        "- guardar y cargar modelos entrenados en Pytorch\n"
      ],
      "metadata": {
        "id": "WOisqYo5q8mP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"impacto-climatico\"></a>\n",
        "# Impacto Climático\n",
        "Un [informe](https://www.wri.org/insights/7-things-know-about-ipccs-special-report-climate-change-and-land) del Instituto de Recursos Mundiales (WRI) indica que aproximadamente el 23% de las emisiones globales de gases de efecto invernadero (GEI) causadas por el ser humano provienen de usos de la tierra como la agricultura, la silvicultura y la expansión urbana. Los cambios en el uso del suelo, como la deforestación y la degradación de la tierra, están entre los principales impulsores de estas emisiones. La urbanización rápida, que genera un aumento de áreas construidas, así como una pérdida masiva de almacenamiento de carbono terrestre, también puede resultar en emisiones significativas de carbono.\n",
        "\n",
        "Mapear la extensión de las categorías de uso y cobertura del suelo a lo largo del tiempo es esencial para un mejor monitoreo ambiental, la planificación urbana y la protección de la naturaleza. Por ejemplo, monitorear los cambios en la cobertura forestal e identificar los impulsores de la pérdida de bosques puede ser útil para los esfuerzos de conservación y restauración forestal. Evaluar la vulnerabilidad de ciertos tipos de cobertura del suelo, como asentamientos y tierras agrícolas, a ciertos riesgos también puede ser útil para la planificación de la reducción del riesgo de desastres, así como para los esfuerzos de adaptación climática a largo plazo.\n",
        "\n",
        "Con la creciente disponibilidad de datos de observación terrestre y los recientes avances en visión por computadora, la inteligencia artificial (IA) y la observación terrestre (EO) han allanado el camino para mapear el uso y cobertura del suelo a una escala sin precedentes. En este tutorial, exploraremos el uso de imágenes satelitales Sentinel-2 y modelos de aprendizaje profundo en Pytorch para automatizar el mapeo de LULC.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<p><img src=\"https://ptes.org/wp-content/uploads/2018/04/iStock-664630460-e1524839082464.jpg\" alt=\"Impacto climático\" width=\"50%\"/></p>\n",
        "</center>\n"
      ],
      "metadata": {
        "id": "n2vFHgukrINX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"publico-objetivo\"></a>\n",
        "# Público Objetivo\n",
        "Este tutorial está dirigido a científicos de datos experimentados y aspirantes que buscan ejemplos concretos de las aplicaciones del aprendizaje profundo en el cambio climático. Esperamos que los usuarios tengan algún conocimiento previo en aprendizaje automático, incluyendo redes neuronales. ¡Pero no te preocupes si eres nuevo en estos temas! Proporcionaremos recursos adicionales y enlaces externos más adelante para que puedas estudiar y profundizar en estos conceptos.\n"
      ],
      "metadata": {
        "id": "RgN4TqpQrWZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"antecedentes-y-requisitos-previos\"></a>\n",
        "## Antecedentes y Requisitos Previos\n",
        "Si necesitas repasar sobre redes neuronales, no dudes en revisar el video a continuación de [3Blue1Brown](https://www.youtube.com/c/3blue1brown).\n",
        "\n",
        "También recomendamos encarecidamente la [colección de conferencias de Stanford](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv) sobre redes neuronales convolucionales (CNNs) para reconocimiento visual. Los cursos de especialización en aprendizaje profundo disponibles en [deeplearning.ai](https://www.deeplearning.ai/courses/) también ofrecen una introducción detallada a las redes neuronales artificiales (ANNs), CNNs, modelos de secuencia y otros conceptos de aprendizaje profundo.\n"
      ],
      "metadata": {
        "id": "YxBYXTw6rjs2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('P28LKWTzrI')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "IAEGZ4XPqTzm",
        "outputId": "92a651bb-b60a-4eed-e556-fcf27a134868"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7d3e5ceb67d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/P28LKWTzrI\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"requisitos-de-software\"></a>\n",
        "# Requisitos de Software\n",
        "\n",
        "Este notebook requiere Python >= 3.7. Las siguientes bibliotecas son necesarias:\n",
        "*   tqdm\n",
        "*   pandas\n",
        "*   numpy\n",
        "*   matplotlib\n",
        "*   pytorch"
      ],
      "metadata": {
        "id": "D9FgkGVKsXNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activar GPU en Google Colab\n",
        "Antes de comenzar, necesitarás acceso a una GPU. Afortunadamente, Google Colab proporciona acceso gratuito a recursos de computación, incluidas GPUs. Las GPUs disponibles actualmente en Colab incluyen Nvidia K80s, T4s, P4s y P100s. Lamentablemente, no es posible elegir el tipo de GPU al que te puedes conectar en Colab. [Consulta aquí para más información](https://research.google.com/colaboratory/faq.html#gpu-availability).\n",
        "\n",
        "Para habilitar la GPU en Google Colab:\n",
        "1. Ve a **Editar→Configuración del Notebook** o **Entorno de ejecución→Cambiar tipo de entorno de ejecución**.\n",
        "2. Selecciona **GPU** en el menú desplegable de Acelerador de hardware."
      ],
      "metadata": {
        "id": "_zk8i1XBsbI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lbpUqRPksdkW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vP75F0FlorVd"
      },
      "outputs": [],
      "source": [
        "# Librerías estándar\n",
        "import os\n",
        "import random\n",
        "from tqdm.notebook import tqdm  # Barra de progreso en Jupyter Notebooks\n",
        "\n",
        "# Manipulación y visualización de datos\n",
        "import matplotlib.pyplot as plt  # Para gráficos y visualización\n",
        "from PIL import Image  # Manipulación de imágenes\n",
        "import seaborn as sns  # Visualización avanzada\n",
        "import pandas as pd  # Manipulación de datos estructurados\n",
        "import numpy as np  # Operaciones matemáticas y manejo de matrices\n",
        "\n",
        "# Librerías de aprendizaje profundo\n",
        "import torch  # Biblioteca principal de PyTorch\n",
        "import torchvision  # Herramientas de visión por computadora en PyTorch\n",
        "import torchsummary  # Resumen de modelos de PyTorch\n",
        "from torch.utils import data  # Utilidades para el manejo de datos\n",
        "from torchvision import datasets, models, transforms  # Conjuntos de datos, modelos y transformaciones\n",
        "\n",
        "# Establecer semilla para reproducibilidad\n",
        "SEED = 42\n",
        "np.random.seed(SEED)  # Fija la semilla para operaciones aleatorias de numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google Colab GPU\n",
        "Check that the GPU  enabled in your colab notebook by running the cell below."
      ],
      "metadata": {
        "id": "wdeyc0ilsvlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check is GPU is enabled\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device: {}\".format(device))\n",
        "\n",
        "# Get specific GPU model\n",
        "if str(device) == \"cuda:0\":\n",
        "  print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5fiEZz_s0Pk",
        "outputId": "35ccb8a4-11d7-44e6-81be-efdd3f592ab8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda:0\n",
            "GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montar Drive\n",
        "\n",
        "Montar el Drive permitirá que el notebook de Google Colab cargue y acceda a archivos desde tu Google Drive.\n"
      ],
      "metadata": {
        "id": "Ev0LQk21tJJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-n_DTuHtT4a",
        "outputId": "fe0a8bd6-4a5f-49c2-ba9d-9f9ab107c885"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"descripcion-de-los-datos\"></a>\n",
        "# Descripción de los Datos\n",
        "\n",
        "En esta sección, aprenderás a:\n",
        "- Descargar el conjunto de datos EuroSAT en tu Google Drive\n",
        "- Generar los conjuntos de entrenamiento y prueba dividiendo el conjunto de datos EuroSAT\n",
        "- Visualizar una muestra de las imágenes y sus etiquetas de uso y cobertura del suelo (LULC)\n"
      ],
      "metadata": {
        "id": "EVi9482dtimJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto de Datos EuroSAT\n",
        "El [conjunto de datos EuroSAT](https://github.com/phelber/EuroSAT) contiene 27,000 imágenes satelitales Sentinel-2 etiquetadas, de 64x64 píxeles, distribuidas en 10 categorías diferentes de uso y cobertura del suelo (LULC). Están disponibles tanto imágenes RGB como multiespectrales (MS) para su descarga. Para simplificar, nos enfocaremos en la clasificación de imágenes RGB.\n"
      ],
      "metadata": {
        "id": "BhXM5WkatjkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://madm.dfki.de/files/sentinel/EuroSAT.zip -O EuroSAT.zip\n",
        "!unzip -q EuroSAT.zip -d 'EuroSAT/'\n",
        "!rm EuroSAT.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRlWtwvjtsFi",
        "outputId": "47f7f076-d035-41bb-d80f-e58efbd9be24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-25 19:06:18--  http://madm.dfki.de/files/sentinel/EuroSAT.zip\n",
            "Resolving madm.dfki.de (madm.dfki.de)... 131.246.195.183\n",
            "Connecting to madm.dfki.de (madm.dfki.de)|131.246.195.183|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94280567 (90M) [application/zip]\n",
            "Saving to: ‘EuroSAT.zip’\n",
            "\n",
            "EuroSAT.zip         100%[===================>]  89.91M   152KB/s    in 11m 41s \n",
            "\n",
            "2024-11-25 19:18:00 (131 KB/s) - ‘EuroSAT.zip’ saved [94280567/94280567]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generar Conjuntos de Entrenamiento y Prueba\n",
        "\n",
        "### Crear una Clase de Conjunto de Datos Personalizado\n",
        "En Pytorch, la clase `Dataset` te permite definir una clase personalizada para cargar las entradas y etiquetas de un conjunto de datos. Utilizaremos esta capacidad para cargar nuestras entradas en forma de imágenes satelitales RGB junto con sus etiquetas correspondientes. Más adelante aprenderemos cómo aplicar las transformaciones necesarias a las imágenes (ver la siguiente sección).\n"
      ],
      "metadata": {
        "id": "IuJu62IAt2Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EuroSAT(data.Dataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Apply image transformations\n",
        "        if self.transform:\n",
        "            x = self.transform(dataset[index][0])\n",
        "        else:\n",
        "            x = dataset[index][0]\n",
        "        # Get class label\n",
        "        y = dataset[index][1]\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(dataset)"
      ],
      "metadata": {
        "id": "b0xQNh1GtxhL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aumento de Datos\n",
        "\n",
        "El aumento de datos es una técnica que aplica aleatoriamente transformaciones a las imágenes, como recortes, volteos horizontales y volteos verticales, durante el entrenamiento del modelo. Estas perturbaciones reducen el sobreajuste de la red neuronal al conjunto de datos de entrenamiento y le permiten generalizar mejor al conjunto de prueba no visto.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/0*ttoU2HOnBI8cb9Y2.png\" width=\"400\"/>\n",
        "<br>\n",
        "<font size=2>Fuente de la Imagen: <a href=\"https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010\">https://pranjal-ostwal.medium.com/data-augmentation-for-computer-vision-b88b818b6010</a></font>\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "<font size=2>Fuente de la Imagen: Ahmad, Jamil & Muhammad, Khan & Baik, Sung. (2017). Data augmentation-assisted deep learning of hand-drawn partially colored sketches for visual search. PLOS ONE. 12. e0183838. 10.1371/journal.pone.0183838. </font>\n",
        "\n",
        "---\n",
        "\n",
        "### Normalización de Imágenes\n",
        "\n",
        "Adicionalmente, en la celda siguiente, el método `transforms.Normalize` normaliza cada uno de los tres canales de las imágenes a los valores de media y desviación estándar definidos en las variables `imagenet_mean` y `imagenet_std`. ImageNet es un conjunto de datos masivo de imágenes y etiquetas. Más adelante en este tutorial, utilizaremos un modelo preentrenado con este conjunto de datos. Para usar este modelo preentrenado en nuestro conjunto de datos LULC, es necesario garantizar que el conjunto de datos de entrada esté normalizado para tener las mismas estadísticas (media y desviación estándar) que ImageNet.\n",
        "\n",
        "<br>\n",
        "<center>\n",
        "<img src=\"https://cv.gluon.ai/_images/imagenet_banner.jpeg\" width=\"400\"/>\n",
        "<br>\n",
        "<font size=2>Fuente de la Imagen: <a href=\"https://cv.gluon.ai/build/examples_datasets/imagenet.html\">https://cv.gluon.ai/build/examples_datasets/imagenet.html</a></font>\n",
        "</center>\n",
        "\n",
        "La investigación existente ha encontrado que usar modelos preentrenados en conjuntos de datos masivos, como ImageNet, mejora la precisión al aplicar estas redes neuronales a nuevos conjuntos de datos. Los modelos preentrenados sirven como excelentes extractores de características genéricas. [Puedes leer más al respecto aquí](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html).\n"
      ],
      "metadata": {
        "id": "kb_t0TI9t49C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 224\n",
        "imagenet_mean, imagenet_std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(input_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(input_size),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(imagenet_mean, imagenet_std)\n",
        "])"
      ],
      "metadata": {
        "id": "pLT1zL-8uKyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar el Conjunto de Datos EuroSAT\n",
        "Comencemos cargando el conjunto de datos EuroSAT utilizando la clase `ImageFolder` de torch.\n",
        "\n",
        "`ImageFolder` es un cargador de datos genérico donde las imágenes están organizadas de la siguiente manera:\n",
        "\n",
        "```\n",
        "    data\n",
        "    └───AnnualCrop\n",
        "    │   │   AnnualCrop_1.jpg\n",
        "    │   │   AnnualCrop_2.jpg\n",
        "    │   │   AnnualCrop_3.jpg\n",
        "    │   │   ...\n",
        "    └───Forest\n",
        "    │   │   Forest_1.jpg\n",
        "    │   │   Forest_2.jpg\n",
        "    │   │   Forest_3.jpg\n",
        "    │   │   ...\n",
        "```"
      ],
      "metadata": {
        "id": "3M0zGc9kuLHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data_dir = './EuroSAT/2750/'\n",
        "dataset = datasets.ImageFolder(data_dir)\n",
        "\n",
        "# Get LULC categories\n",
        "class_names = dataset.classes\n",
        "print(\"Class names: {}\".format(class_names))\n",
        "print(\"Total number of classes: {}\".format(len(class_names)))"
      ],
      "metadata": {
        "id": "vBwpiE4yuXYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dividir en Conjuntos de Entrenamiento, Validación y Prueba\n",
        "Aquí dividimos el conjunto de datos en un conjunto de entrenamiento y un conjunto de prueba. El conjunto de entrenamiento será el 70% del conjunto de datos EuroSAT, seleccionado aleatoriamente. Reservamos el 15% del conjunto de datos como nuestro conjunto de validación y el 15% restante como nuestro conjunto de prueba.\n"
      ],
      "metadata": {
        "id": "keey5whdyE6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply different transformations to the training and test sets\n",
        "train_data = EuroSAT(dataset, train_transform)\n",
        "val_data = EuroSAT(dataset, val_transform)\n",
        "test_data = EuroSAT(dataset, test_transform)\n",
        "\n",
        "# Randomly split the dataset into 70% train / 15% val / 15% test\n",
        "# by subsetting the transformed train and test datasets\n",
        "train_size = 0.70\n",
        "val_size = 0.15\n",
        "indices = list(range(int(len(dataset))))\n",
        "train_split = int(train_size * len(dataset))\n",
        "val_split = int(val_size * len(dataset))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_data = data.Subset(train_data, indices=indices[:train_split])\n",
        "val_data = data.Subset(val_data, indices=indices[train_split: train_split+val_split])\n",
        "test_data = data.Subset(test_data, indices=indices[train_split+val_split:])\n",
        "print(\"Train/val/test sizes: {}/{}/{}\".format(len(train_data), len(val_data), len(test_data)))"
      ],
      "metadata": {
        "id": "nc-V3HZiyF9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, utilizamos la clase `DataLoader` de `torch` para crear un cargador de datos.  \n",
        "El cargador de datos gestiona la obtención de muestras de los conjuntos de datos (incluso puede obtenerlas en paralelo utilizando `num_workers`) y ensambla lotes (batches) de los conjuntos de datos.\n"
      ],
      "metadata": {
        "id": "hnD-SB21yIRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 2\n",
        "batch_size = 16\n",
        "\n",
        "train_loader = data.DataLoader(\n",
        "    train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True\n",
        ")\n",
        "val_loader = data.DataLoader(\n",
        "    val_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")\n",
        "test_loader = data.DataLoader(\n",
        "    test_data, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "LE-XnCbWyPUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizar Datos\n",
        "\n",
        "En la celda a continuación, visualizaremos un lote (batch) del conjunto de datos. La celda muestra las entradas a la red neuronal (la imagen RGB) junto con la etiqueta asociada."
      ],
      "metadata": {
        "id": "Wj0uYj_qyPEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "inputs, classes = next(iter(train_loader))\n",
        "fig, axes = plt.subplots(n, n, figsize=(8, 8))\n",
        "\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    image = inputs[i * n + j].numpy().transpose((1, 2, 0))\n",
        "    image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
        "\n",
        "    title = class_names[classes[i * n + j]]\n",
        "    axes[i, j].imshow(image)\n",
        "    axes[i, j].set_title(title)\n",
        "    axes[i, j].axis('off')"
      ],
      "metadata": {
        "id": "fGGohjc0yYVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análisis Exploratorio de Datos\n",
        "\n",
        "A continuación, exploremos un poco más nuestro conjunto de datos. En particular, ¿cuántas imágenes de cada clase están incluidas?\n"
      ],
      "metadata": {
        "id": "pJr-ybvLyXzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 3))\n",
        "hist = sns.histplot(dataset.targets)\n",
        "\n",
        "hist.set_xticks(range(len(dataset.classes)))\n",
        "hist.set_xticklabels(dataset.classes, rotation=90)\n",
        "hist.set_title('Histogram of Dataset Classes in EuroSAT Dataset')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DfK1BOFgygAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desarrollo del Modelo\n",
        "\n",
        "## Instanciar el Modelo\n",
        "\n",
        "Primero, instanciemos el modelo. Para comenzar, utilizaremos una arquitectura estándar de redes neuronales, llamada ResNet-50. Según el [trabajo de Helber et al.](https://arxiv.org/pdf/1709.00029.pdf), se ha demostrado que ResNet-50 funciona bien para la clasificación de uso y cobertura del suelo (LULC) en el conjunto de datos EuroSAT.\n",
        "\n",
        "### ResNet-50\n",
        "<b>Recuerda</b>: Las redes neuronales profundas son difíciles de entrenar debido al problema de gradientes que se desvanecen o explotan (la multiplicación repetida hace que el gradiente sea infinitamente pequeño). ResNet resuelve este problema utilizando conexiones de atajo que conectan activaciones de una capa anterior con una capa posterior, saltando una o más capas, como se muestra a continuación. Esto permite que los gradientes se propaguen a las capas más profundas antes de que se reduzcan a valores pequeños o cero.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/images/resnet50.png\" width=\"600\"/><br>\n",
        "Fuente de la Imagen: <a href=\"https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/\">https://jananisbabu.github.io/ResNet50_From_Scratch_Tensorflow/</a>\n",
        "</center>\n",
        "<br>\n",
        "\n",
        "Nota que cuando cargamos el modelo, configuramos `weights=models.ResNet50_Weights.DEFAULT` para indicar que el modelo cargado ya debe estar preentrenado en el conjunto de datos ImageNet. También modificamos la capa final para que la salida coincida con el número de clases en nuestro conjunto de datos.\n"
      ],
      "metadata": {
        "id": "Y-J328ubym01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "FkK8UYPdyrqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento y Evaluación del Modelo\n",
        "\n",
        "Ahora podemos proceder con el entrenamiento y la evaluación del modelo.\n",
        "\n",
        "Esta sección tiene tres partes principales:\n",
        "\n",
        "1. Especificar el criterio, el optimizador y los hiperparámetros (por ejemplo, número de épocas, tasa de aprendizaje, etc.).\n",
        "2. Entrenar el modelo en el conjunto de entrenamiento actualizando sus pesos para minimizar la función de pérdida.\n",
        "3. Evaluar el modelo en el conjunto de prueba para observar su desempeño en datos nuevos y no vistos.\n",
        "4. Repetir los pasos 2 y 3 durante `n_epochs` veces.\n"
      ],
      "metadata": {
        "id": "DuqXeBG4y4Su"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pérdida de Entropía Cruzada\n",
        "Definimos nuestra función de pérdida como la pérdida de entropía cruzada, que mide el rendimiento de un modelo de clasificación cuyo resultado es un valor de probabilidad entre 0 y 1. La pérdida de entropía cruzada aumenta a medida que la probabilidad predicha se desvía de la etiqueta real. ([Fuente](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html))\n",
        "\n",
        "Para dos clases, se calcula como:\n",
        "\n",
        "$−y\\log(p)−(1−y)\\log(1−p)$\n",
        "\n",
        "Para la clasificación multiclase con $M$ clases, se define como:\n",
        "\n",
        "$−\\sum_{c=1}^{M}y_{o,c}\\log(p_{o,c})$\n",
        "\n",
        "donde:\n",
        "\n",
        "- $M$ - número de clases (ejemplo: perro, gato, pez)\n",
        "- $log$ - logaritmo natural\n",
        "- $y_{o,c}$ - indicador binario (0 o 1) que especifica si la clase $c$ es la clasificación para la observación $o$\n",
        "- $p_{o,c}$ - probabilidad predicha de que la observación $o$ pertenece a la clase $c$\n",
        "\n",
        "---\n",
        "\n",
        "### Descenso de Gradiente Estocástico (SGD)\n",
        "Recuerda que el objetivo del descenso de gradiente estocástico (SGD) es minimizar la función de pérdida. Para lograr esto, calcula la pendiente (gradiente) de la función de pérdida en el punto actual y se mueve en la dirección opuesta de la pendiente hacia el descenso más pronunciado.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://miro.medium.com/max/1400/1*P7z2BKhd0R-9uyn9ThDasA.png\" width=\"350\"/><br>\n",
        "Fuente de la Imagen: <a href=\"https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a\">https://towardsdatascience.com/batch-mini-batch-stochastic-gradient-descent-7a62ecba642a</a>\n",
        "</center>\n",
        "<br>\n"
      ],
      "metadata": {
        "id": "hsYZubG3zF3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify number of epochs and learning rate\n",
        "n_epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# Specify criterion and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "k_RsFfocy61G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, vamos a crear nuestra función de entrenamiento."
      ],
      "metadata": {
        "id": "HYiQYcb7zONW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer):\n",
        "  model.train()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_total_correct = 0.0\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    # Zero the parameter gradients\n",
        "    # Clear off previous weights in order\n",
        "    # to obtain updated weights.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = criterion(outputs, labels)\n",
        "\n",
        "    # Compute the gradients wrt the loss\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights based on the\n",
        "    # internally stored gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate statistics\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    # Calculate running loss and accuracy\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "  # Calculate epoch loss and accuracy\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "  print(f\"Train Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
        "\n",
        "  return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "4356AQymzOlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, definamos la función de evaluación del modelo."
      ],
      "metadata": {
        "id": "WdLa2Za_zSiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, phase=\"val\"):\n",
        "  model.eval()\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_total_correct = 0.0\n",
        "\n",
        "  for i, (inputs, labels) in enumerate(tqdm(dataloader)):\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    running_loss += loss.item() * inputs.size(0)\n",
        "    running_total_correct += torch.sum(preds == labels)\n",
        "\n",
        "  # Calculate epoch loss and accuracy\n",
        "  epoch_loss = running_loss / len(dataloader.dataset)\n",
        "  epoch_accuracy = (running_total_correct / len(dataloader.dataset)) * 100\n",
        "  print(f\"{phase.title()} Loss: {epoch_loss:.2f}; Accuracy: {epoch_accuracy:.2f}\")\n",
        "\n",
        "  return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "id": "X0VpJnLrzYSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uniendo todo, definimos la función `fit` para entrenar y evaluar el modelo en el conjunto de entrenamiento y el conjunto de validación, respectivamente."
      ],
      "metadata": {
        "id": "XQ6210JszacV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer):\n",
        "  # Keep track of the best loss and\n",
        "  # best model weights with the lowest loss\n",
        "  best_loss = np.inf\n",
        "  best_model = None\n",
        "\n",
        "  # Train and test over n_epochs\n",
        "  for epoch in range(n_epochs):\n",
        "    print(\"Epoch {}\".format(epoch+1))\n",
        "    train(model, train_loader, criterion, optimizer)\n",
        "    val_loss, _ = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "      best_loss = val_loss\n",
        "      best_model = model\n",
        "\n",
        "  return best_model"
      ],
      "metadata": {
        "id": "Om31Fizuzhte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos comenzar con el entrenamiento y la evaluación del modelo en la siguiente celda."
      ],
      "metadata": {
        "id": "20uL9C_dzj1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "utUzpAuAzp_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, solo entrenamos el modelo durante 10 épocas. En la práctica, querrás entrenar el modelo por mucho más tiempo para obtener los mejores resultados.\n"
      ],
      "metadata": {
        "id": "mQfoWVy0z0UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rendimiento del Modelo en el Conjunto de Prueba\n",
        "Usando el mejor modelo de los pasos anteriores, podemos evaluar el rendimiento del modelo en el conjunto de prueba."
      ],
      "metadata": {
        "id": "jrbmtCc-z1WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, _ = evaluate(best_model, test_loader, criterion, phase=\"test\")"
      ],
      "metadata": {
        "id": "XdhJZePfzsPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardar el Modelo\n",
        "\n",
        "Definamos una función para guardar el modelo en nuestro Google Drive local de la siguiente manera."
      ],
      "metadata": {
        "id": "C-8weST2z8e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"./drive/My Drive/Colab Notebooks/models/\"\n",
        "if not os.path.exists(model_dir):\n",
        "  os.makedirs(model_dir)\n",
        "\n",
        "model_file = os.path.join(model_dir, 'best_model.pth')\n",
        "model_file"
      ],
      "metadata": {
        "id": "FQe1MGCNz83d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(best_model, model_file):\n",
        "  torch.save(best_model.state_dict(), model_file)\n",
        "  print('Model successfully saved to {}.'.format(model_file))"
      ],
      "metadata": {
        "id": "Y0WLgkBR0BUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_model(best_model, model_file)"
      ],
      "metadata": {
        "id": "f02yXFQ_0BuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar el Modelo\n",
        "Aquí te mostramos cómo cargar el modelo guardado en el paso anterior."
      ],
      "metadata": {
        "id": "wiJePDBN0EyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_file):\n",
        "  # Uncomment this to download the model file\n",
        "  #if not os.path.isfile(model_file):\n",
        "  #  model_file = 'best_model.pth'\n",
        "  #  !gdown \"13AFOESwxKmexCoOeAbPSX_wr-hGOb9YY\"\n",
        "\n",
        "  model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "  model.fc = torch.nn.Linear(model.fc.in_features, 10)\n",
        "  model.load_state_dict(torch.load(model_file))\n",
        "  model.eval()\n",
        "\n",
        "  print('Model file {} successfully loaded.'.format(model_file))\n",
        "  return model"
      ],
      "metadata": {
        "id": "RzpKCRBA0FmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(model_file)"
      ],
      "metadata": {
        "id": "NafIYg-x0JvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"resultados\"></a>\n",
        "# Resultados\n",
        "\n",
        "Vamos a visualizar un ejemplo de la red neuronal haciendo una predicción."
      ],
      "metadata": {
        "id": "koWLI7qx0Lz-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve sample image\n",
        "index = 15\n",
        "image, label = test_data[index]\n",
        "\n",
        "# Predict on sample\n",
        "model = model.to(\"cpu\")\n",
        "output = model(image.unsqueeze(0))\n",
        "_, pred = torch.max(output, 1)\n",
        "\n",
        "# Get corresponding class label\n",
        "label = class_names[label]\n",
        "pred = class_names[pred[0]]\n",
        "\n",
        "# Visualize sample and prediction\n",
        "image = image.cpu().numpy().transpose((1, 2, 0))\n",
        "image = np.clip(np.array(imagenet_std) * image + np.array(imagenet_mean), 0, 1)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(3, 3))\n",
        "ax.imshow(image)\n",
        "ax.set_title(\"Predicted class: {}\\nActual Class: {}\".format(pred, label));"
      ],
      "metadata": {
        "id": "f4suEDL50VEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí mostramos cómo ejecutar el modelo en una imagen PIL."
      ],
      "metadata": {
        "id": "MynUIFPY0dBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "image_path = './EuroSAT/2750/Forest/Forest_2.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Transform image\n",
        "input = test_transform(image)\n",
        "\n",
        "# Predict on sample\n",
        "output = model(input.unsqueeze(0))\n",
        "\n",
        "# Get corresponding class label\n",
        "_, pred = torch.max(output, 1)\n",
        "pred = class_names[pred[0]]\n",
        "\n",
        "# Visualize results\n",
        "fig, ax = plt.subplots(figsize=(3,3))\n",
        "ax.imshow(image)\n",
        "ax.set_title(\"Predicted class: {}\".format(pred));"
      ],
      "metadata": {
        "id": "-1NKVL8C0d-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"ejercicios\"></a>\n",
        "# Ejercicio 1: Experimenta con una estrategia de ajuste fino diferente.\n",
        "\n",
        "Hasta ahora, hemos inicializado la CNN con pesos de un modelo entrenado en los datos de ImageNet y hemos vuelto a entrenar el modelo en el conjunto de datos EuroSAT actualizando **todos los pesos**. Otro enfoque para el ajuste fino consiste en utilizar las capas convolucionales preentrenadas como un extractor de características fijo y congelar esos pesos, actualizando únicamente los pesos de las capas totalmente conectadas finales para la clasificación.\n"
      ],
      "metadata": {
        "id": "XN4ZH9HS0hG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all layers\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Add final (unfrozen) layer for classification\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "bX-7XrgQ0nQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "AG0MBwZ70m6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Add final (unfrozen) layer for classification\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, len(dataset.classes))\n",
        "model = model.to(device)\n",
        "\n",
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "yu1clJMS0st0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 2: Experimenta con diferentes modelos CNN preentrenados en ImageNet.\n"
      ],
      "metadata": {
        "id": "UzHjOYxN2BLT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ResNet50 es solo una de las muchas arquitecturas de modelos CNN disponibles. Determinar la mejor arquitectura de modelo es una parte importante del proceso de selección de modelos y, en la práctica, es mejor probar y comparar diferentes arquitecturas. Una descripción detallada de los modelos CNN preentrenados compatibles con Pytorch se puede encontrar aquí: [https://pytorch.org/vision/0.18/models.html](https://pytorch.org/vision/0.18/models.html)\n",
        "\n",
        "⭐ **¡TU TURNO!:** Prueba un modelo CNN preentrenado diferente de Pytorch, basado en los modelos disponibles que se encuentran [aquí](https://pytorch.org/vision/0.18/models.html). ¿Qué tan bien se desempeña en comparación con ResNet50?\n",
        "\n",
        "Asegúrate de modificar la última capa para clasificación para que coincida con el número de clases (Pista: el número de clases = `len(dataset.classes)`).\n"
      ],
      "metadata": {
        "id": "bwdCcwSD0w58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ### YOUR CODE HERE ###\n",
        "\n",
        "# Modify the final layer for classification\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "3uOdvGId06m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "tS_PtKmM09FM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "EmZDMDMU09fD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aquí elegimos EfficientNet-B0 como el modelo para entrenar y evaluar. Puedes encontrar más información sobre EfficientNets [aquí](https://arxiv.org/pdf/1905.11946).\n"
      ],
      "metadata": {
        "id": "OicIJcCA1I4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
        "\n",
        "# Modify the final layer for classification\n",
        "model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, len(dataset.classes))\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "v21SVBwB1AVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "cytaznjo1Foj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3: Experimenta con Modelos CNN Preentrenados en Imágenes Satelitales\n",
        "\n",
        "Tradicionalmente, los profesionales han confiado en modelos preentrenados en ImageNet para tareas de aprendizaje por transferencia, incluso para datos de observación terrestre. Sin embargo, con la creciente disponibilidad de datos de teledetección, ahora estamos viendo que cada vez más conjuntos de datos de teledetección a gran escala se están curando para el preentrenamiento de modelos, como [SSL4EO-S12](https://github.com/zhu-xlab/SSL4EO-S12) de Wang et al. Los modelos entrenados en estos conjuntos de datos utilizan técnicas de aprendizaje no supervisado/autosupervisado, aprovechando las grandes cantidades de datos de teledetección sin etiquetar para el preentrenamiento de modelos.\n",
        "\n",
        "Estos pesos preentrenados están disponibles a través de la [librería Torchgeo](https://github.com/microsoft/torchgeo). Puedes encontrar más información sobre los pesos preentrenados disponibles aquí: [https://torchgeo.readthedocs.io/en/stable/api/models.html#pretrained-weights](https://torchgeo.readthedocs.io/en/stable/api/models.html#pretrained-weights)\n"
      ],
      "metadata": {
        "id": "HYoyKOP01eIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭐ **¡TU TURNO!:** Usando torchgeo, carga un modelo ResNet50 preentrenado en Sentinel-2 y ajusta el modelo actualizando **todos los pesos**. ¿Cómo se compara este modelo con el modelo ResNet50 preentrenado en ImageNet?\n"
      ],
      "metadata": {
        "id": "GajzwkRx1xUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q timm\n",
        "!pip install -q torchgeo"
      ],
      "metadata": {
        "id": "WtjKKSLd1mDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchgeo.models import ResNet50_Weights\n",
        "\n",
        "model = ### YOUR CODE HERE ###\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "itKYdJn01pk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "anU4kEAT1rRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solución"
      ],
      "metadata": {
        "id": "fSEPqe9c1y8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "from torchgeo.models import ResNet50_Weights\n",
        "\n",
        "weights = ResNet50_Weights.SENTINEL2_RGB_MOCO\n",
        "model = timm.create_model(\n",
        "    \"resnet50\", in_chans=weights.meta[\"in_chans\"],\n",
        "    num_classes=len(dataset.classes)\n",
        ")\n",
        "model.load_state_dict(weights.get_state_dict(progress=True), strict=False)\n",
        "\n",
        "model = model.to(device)\n",
        "torchsummary.summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "id": "R391f6R_1yoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Commence training\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "best_model = fit(model, train_loader, val_loader, n_epochs, lr, criterion, optimizer)"
      ],
      "metadata": {
        "id": "gWKS6DsJ2PLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Felicidades por llegar hasta aquí! Ve a la siguiente sección, [Parte 2: Automatización del Mapeo de Uso y Cobertura del Suelo usando Python.](https://colab.research.google.com/drive/13I1wZT7thBlNdGA1tFQrK1MlRhMZMnpM?usp=sharing).\n"
      ],
      "metadata": {
        "id": "PYfopCEE2Vkr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referencias\n",
        "- Helber, P., Bischke, B., Dengel, A., & Borth, D. (2019). Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(7), 2217-2226.\n",
        "- Wang, Y., Braham, N. A. A., Xiong, Z., Liu, C., Albrecht, C. M., & Zhu, X. X. (2023). SSL4EO-S12: A large-scale multimodal, multitemporal dataset for self-supervised learning in Earth observation [Software and Data Sets]. IEEE Geoscience and Remote Sensing Magazine, 11(3), 98-106.\n"
      ],
      "metadata": {
        "id": "y_tG_ISP2W9q"
      }
    }
  ]
}
